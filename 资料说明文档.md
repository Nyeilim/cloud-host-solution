# 资料说明文档

# 文献 01 Bubble Up：通过合理的协同定位提高现代仓库级计算机的利用率

## 背景

随着世界上大部分计算继续进入云计算，在现代数据中心中，为确保延迟敏感任务（如网络搜索）的性能隔离而过度规划计算资源是导致机器利用率低的主要原因。由于无法准确预测多核系统上共享资源的争用导致的性能下降，导致了简单地禁止高优先级、延迟敏感的任务与其他任务共处一地的高压方法。执行这种精确的预测一直是一个具有挑战性且尚未解决的问题。

仓库规模计算机”（WSC）[12]，[22]容纳大型 Web 应用程序和云服务。这些数据中心的建设和运营成本从数千万美元到数亿美元不等。随着越来越多的计算转移到云中，尽可能高效地利用 WSC 中的资源变得极其重要。然而，现代 WSC 中计算资源的利用率仍然很低，通常不超过 20%[2]。

数据中心中的每台计算机都包含多个内核，通常每个插槽 4 到 8 个核心，每台计算机 2 到 4 个插槽。但是，鉴于单台计算机上并行性的巨大潜力，内核之间共享了许多资源。这种共享可能会导致内核之间的性能干扰，对面向用户和延迟敏感的应用程序线程的服务质量 （QoS） 产生负面且不可预测的影响 [36]。为避免潜在的干扰，不允许对延迟敏感的应用程序使用共置，从而使核心处于空闲状态，并导致过度预配，从而对整个数据中心的利用率产生负面影响。

## 任务

这种过度预配通常是不必要的，因为共置可能会也可能不会导致严重的性能干扰。**这项工作的目标是能够精确预测由于内存子系统中的共享资源争用而导致的性能下降。精确预测是在位于同一位置时提供预期性能损失量的预测。使用此信息，可以允许不违反应用程序 QoS 阈值的共置，从而提高数据中心的利用率。**

## 方法

在本文中，我们介绍了 Bubble Up，这是一种表征方法，**能够准确预测内存子系统中共享资源争用导致的性能下降。通过使用气泡向生产数据中心处理器上的内存子系统施加可调的“压力”，我们的方法可以预测同位置应用程序之间的性能干扰，准确率在实际性能下降的 1% 至 2% 以内。**使用这种方法在谷歌的生产数据中心与现实世界中的大型应用程序中实现“合理”的协同定位，我们可以将 500 机器集群的利用率提高 50% 至 90%，同时保证对延迟敏感的应用程序的高质量服务。

**Bubble Up 的关键见解是，预测协同运行应用程序的性能干扰可以分解为 1）测量应用程序生成的内存子系统上的压力，以及 2）测量不同级别的压力对应用程序的影响。基本假设是压力和灵敏度都可以使用通用的压力度量进行量化。具有这样的度量降低了同位置分析的复杂性。**与分析和表征每一个可能的成对共定位的暴力方法相反，Bubble Up 只需要表征每个应用一次，就可以产生精确的成对干扰预测（例如 O（N））。

冒泡是一个分两步的表征过程。首先，针对膨胀的气泡对每个应用程序进行测试，以产生灵敏度曲线。气泡是一种为内存子系统精心设计的压力测试，它为整个系统施加的压力提供了一个“刻度盘”

## 具体贡献如下

我们介绍了 Bubble Up 的设计，这是一种通用的表征方法，能够精确预测任意应用程序在同一位置时所遭受的性能退化。

我们介绍了 17 个谷歌生产工作负载，并描述了它们在生产服务器上共存时对性能干扰的倾向。

除了在 SmashBench 套件中展示我们的 Bubble Up 方法对有争议内核的预测准确性外，我们还评估了在生产数据中心环境中应用 Bubble Up 方法来引导谷歌应用程序成对共处时的预测准确性和利用率的提高。

使用 Bubble Up，我们能够准确预测由于谷歌应用程序的任意位置导致的性能下降，误差最多为 2.2%，通常小于 1%。为了评估使用 Bubble Up 来引导生产工作负载的 QoS 强制共存，我们在一个 500 台机器的集群中进行了一项研究，并能够将集群中的机器利用率提高 50%-90%，这取决于延迟敏感应用程序的允许 QoS 阈值。

## 结论

在这项工作中，我们提出了一种新的方法，用于精确预测由共享资源争用导致的性能下降。这种机制在新兴的仓库规模计算领域尤为重要。通过将应用程序对内存子系统有争议的压力的敏感性及其在子系统中产生的压力的敏感性解耦，我们能够预测成对共址，平均预测误差仅为 1%。在我们的实验群集设置中使用 Bubble- Up，我们能够将数据中心的利用率提高 50% 到 90%，同时实施一系列 QoS 策略。

# 文献 02 DeepDive：透明地识别和管理虚拟环境中的性能干扰

## 背景

许多企业和个人已经将工作负载转移到基础设施即服务（IaaS）提供商，如亚马逊和 Rackspace。云计算扩展的一个关键因素是虚拟化技术。IaaS 提供商使用虚拟化来（1）将每个客户的应用程序打包到一个或多个虚拟机（VM）中，（2）隔离行为不端的应用程序，（3）通过在多个 VM 中多路复用其物理机（PM）来降低运营成本，以及（4）简化 VM 在 PM 之间的放置和迁移。

尽管虚拟化有很多好处，包括它能够在 CPU 和内存空间分配方面很好地分割 PM，但在这些环境中，性能隔离还远远不够完美。具体来说，提供商面临的一个挑战是识别（和管理）位于每个 PM 的虚拟机之间的性能干扰。例如，两个虚拟机在一起运行时可能会在共享硬件缓存中颠簸，但当每个虚拟机都单独运行时，它们很好地适应了共享硬件缓存。另一个例子是，两个虚拟机在隔离运行时各自具有顺序磁盘 I/O，在一起运行时可能会在共享磁盘上产生随机访问模式。更糟糕的是，技术趋势指向拥有数百甚至数千个核心的多核 PM。在这些 PM 上，遇到干扰的机会将增加。

干扰会严重削弱客户对云提供可预测性能能力的信任。因此，干扰可能会成为吸引对性能敏感的客户的绊脚石。

有效处理干扰具有挑战性，原因有很多。首先，IaaS 提供商忽视了客户的应用程序和工作负载，并且无法轻易确定干扰正在发生。

此外，IaaS 提供商不能依赖应用程序来报告其性能水平（从而知道何时发生干扰），因为这可能会让应用程序开发人员负担过重，他们也不可信。这一挑战与不透明的方法背道而驰[12，18，25，26，27，33，37]。其次，干扰本质上很复杂，可能是由于任何服务器组件（例如，共享硬件缓存、内存、I/O）造成的。

此外，只有当共存的虚拟机同时竞争硬件资源时，干扰才会显现出来。预测性能下降的现有方法[12，18，25，26，37]不适用，因为它们要求提供商在部署前长时间访问位于同一位置的虚拟机。干扰检测必须是一种更快的在线活动。最后，大型公共提供商每天部署的大量新虚拟机可能会导致可扩展性问题。

## 任务

我们描述了 DeepDive 的设计和实现，该系统用于**透明地识别和管理基础设施即服务云环境中位于同一物理机器上的虚拟机（VM）之间的性能干扰。**

## 方法

DeepDive 成功地解决了几个重要挑战，包括缺乏来自应用程序的性能信息，以及详细干扰分析的巨大开销。我们首先**展示了使用易于获取的低级别指标来清楚地辨别干扰何时发生以及是什么资源造成的是可能的。接下来，使用现实的工作负载，我们展示了 DeepDive 可以快速了解共存虚拟机之间的干扰。最后，我们展示了 DeepDive 在检测到干扰时有效处理干扰的能力，通过使用低开销的方法来识别减轻干扰的 VM 位置。**

鉴于这些挑战，提出了 DeepDive，这是一个透明高效地识别和管理 IaaS 提供商干扰的系统。

## 贡献

1. 一种用于透明地获得关于干扰的基本事实的方法，包括应用行为的黑匣子检测以及仅使用低级别度量来精确定位干扰的罪魁祸首资源的能力。
2. 一种警告系统，通过了解正常的、非干扰的行为来减少详细干扰分析的开销。
3. 一种利用全局信息来提高可扩展性的技术，该技术使用在其他 PM 上运行相同工作负载的 VM 的行为。
4. 一种透明且廉价地迁移罪魁祸首虚拟机的机制，通过使用简单的合成基准来模拟虚拟机的低级别行为及其在实际迁移之前对其他虚拟机的影响。
5. 使用实际工作负载的结果显示：  
    i）DeepDive 以高精度透明地推断性能损失（平均误差小于 5%），inference，并精确定位罪魁祸首资源；  
    ii）它是高度准确的（没有假阴性）并且具有低开销（很少的仿形机）；  
    iii）它做出快速（不到一分钟）和准确的 VM 放置决策。

据我们所知，DeepDive 是第一个透明高效地处理对任何主要服务器资源（包括 I/O）的干扰的端到端系统。它的部署将有两个主要好处。首先，它将使云提供商能够使用更少的资源来实现其服务级别目标，这将提高用户满意度并降低能源成本。其次，更智能的虚拟机布局将使云客户能够从提供商那里购买更少的资源。

# 文献 03 PACMan：性能感知虚拟机整合

## 背景

许多数据中心的平均服务器利用率很低，估计在 5%-15%[10]之间。这是浪费的，因为空闲服务器通常消耗其峰值功率的 50% 以上[11]，这意味着低利用率的服务器消耗的能量明显高于高利用率的较少服务器。此外，低利用率意味着要使用更多的服务器，从而导致资本浪费。防止这种浪费的一个解决方案是在更少的服务器上整合应用程序。

整合不可避免地会引入资源争用，从而导致性能下降。为了减少这种争用，数据中心对资源进行虚拟化，并将它们拆分到整合在共享硬件上的应用程序中。但是，虚拟化并不能防止所有形式的争用，因此并不能完全消除性能下降。特别是，共享缓存和内存带宽中的争用会显著降低性能，这是针对各种工作负载测量的[3-5，16，17，19，21，32，35]。执行时间增加了几十个百分点。

为了减少退化，先前的工作已经测量了可能的 VM 组合的退化，然后将导致最小退化的 VM 共同定位[17，18，29]。但这种方法不尊重目标性能约束。性能对于互联网服务来说往往是至关重要的。对亚马逊、微软和谷歌服务的测量显示，延迟增加几分之一秒可能导致高达 1% 至 20% 的收入损失[13，20，26]。下意识的反应是放弃从整合中节省的全部或部分。例如，在谷歌数据中心，整合的工作负载仅使用 50% 的处理器核心[21]。每隔一个处理器核心都不使用，只是为了确保性能不会降低。

## 任务

我们希望保留整合虚拟机的性能，但不要在这样做时浪费过多的资源。挑战在于（1）确定每个虚拟机在与不同的待整合虚拟机集放置时会降低多少，以及（2）确定可以在服务器上放置哪些虚拟机和多少虚拟机，以保持所需的性能。

## 方法

识别合适的 VM 的问题被证明是 NP 完全的，我们设计了一种计算高效的算法，我们证明该算法的性能接近理论最优。因此，我们的方法中未使用的多余资源明显低于目前的做法。

具体而言，我们做出了以下

## 贡献

首先，我们提出了一个具有性能意识的整合管理器 PACMan，它可以最大限度地降低资源成本，如能耗或使用的服务器数量。PACMan 整合虚拟机，使性能下降保持在指定范围内。由于这个问题是 NP 完全的，PACMan 使用了一种近似但计算高效的算法，我们证明该算法在对数上接近最优。

其次，当面向客户的应用程序优先考虑性能时，批处理过程（如 Map Reduce[8]）可能会优先考虑资源效率。对于这种情况，PACMan 提供了一种“Eco”模式，可以填充所有服务器核心，并最大限度地减少最坏情况下的降级。我们特别考虑最坏的情况，而不是[17]中考虑的平均值，因为在 Map Reduce 中，在所有地图任务完成之前，Reduce 无法启动，因此，只有最坏的地图任务的降级才重要。我们表明，很难为这种情况设计可证明的近似最优方法，并提出合适的启发式方法。

最后，我们使用 SPEC CPU 2006 应用程序上测量的降解情况来评估 PACMan。为了在保持性能的同时最大限度地减少浪费的资源，PACMan 在最佳方案的 10% 以内运行，与不考虑干扰的合并方案相比，节省了 30% 以上的能源，并且与当前实践相比，将总运营成本提高了 22%。对于 Eco 模式，与传统方法相比，PACMan 的降解率降低了 52%。

# 文献 04 Bubble-flux：精确的在线 QoS 管理可提高仓库规模计算机的利用率

这篇文章探讨了提供更高机房服务器利用率的同时,也保证延迟敏感应用的服务质量(QoS)的问题。

作者提出了 Bubble-Flux 机制来解决这个问题。Bubble-Flux 包含两个主要组件:动态 Bubble 和在线 Flux 引擎。

动态 Bubble 可以实时地在服务器上进行轻量级的应用特性检测,生成延迟敏感应用当前的服务质量敏感度曲线。这使得调度器可以利用该曲线更准确地预测并发运行作业对服务质量的影响,从而选择更多“安全”的并发作业来提高服务器利用率。

在线 Flux 引擎使用了“淡入/淡出”机制来动态地监测和控制批处理作业的执行,以保证延迟敏感应用的服务质量。一旦监测到服务质量下降,它会通过加快淡出批处理作业来防止服务质量违规。Flux 引擎也能逐步淡入之前未见过的新作业,保证服务质量的同时也增加利用率。

两者结合可以实时获取延迟敏感应用当前的服务质量敏感度,基于此更好地选择并发作业,使用 Flux 引擎动态调整作业执行来适应变化,从而实现机房服务器利用率的提升同时也保证服务质量。作者通过实验表明,Bubble-Flux 机制在保证服务质量的同时,服务器利用率的提升能达到先前静态方法的 2.2 倍。

# 文献 05 Stay-Away：保护敏感应用程序免受性能干扰

这篇文章介绍了一个名为 Stay-Away 的机制,用于在与批处理应用共存环境下保护敏感应用的性能不受影响。

主要内容包括:

1. 虚拟机资源共享虽然可以提高利用率,但也会造成不同虚拟机之间的性能互相影响,这对 QoS 敏感的应用来说是一个难题。
2. Stay-Away 采用一个三步机制来解决这个问题:定期采样各虚拟机的资源指标,将其映射到二维状态空间,并预测执行是否会接近违反 QoS 的状态,必要时通过限制批处理应用来避免这种情况。
3. 它使用 MDS 技术将高维指标映射到二维空间,保留指标之间的相对距离关系,并标记出过去观测到的违反 QoS 状态。
4. 它预测未来状态的可能移动轨迹,如果预测将靠近违反状态则限制批处理应用,同时增加探索空间以逐步学习新的状态。
5. 它通过监控敏感应用在限制批处理应用后状态的变化来判断应该恢复批处理应用的时机。
6. 实验结果表明,Stay-Away 可以很好地保证敏感应用的 QoS,同时通过有效调度提高资源利用率。

总的来说,它提供了一种通用和自适应的机制来在共存环境下保护敏感应用不受性能影响,而无需过于保守地隔离它们。

# 文献 06 应用程序减速模型：量化和控制共享缓存和主内存的应用程序间干扰的影响

这篇文章提出了应用程序延迟模型(ASM),这是一个在线模型,可以准确估算应用程序由于共享缓存容量和主存储带宽干扰而产生的延迟。

ASM 的主要思想是:应用程序的性能大致成比例于它访问共享缓存的速率。即缓存访问率可以作为应用程序性能的一个代理指标。因此,ASM 将应用程序延迟定义为缓存访问率当应用独立运行/缓存访问率当应用共享运行的比率。

ASM 的一个关键挑战是如何准确估算应用程序独立运行时的缓存访问率(CARalone)。为此,ASM 采取了以下两步:

1. 定期将主存储带宽完全给予每一个应用程序,以此来最大程度降低主存储带宽干扰对 CARalone 的影响。
2. 使用辅助标签存储跟踪如果应用独立运行时缓存的状态,来识别哪些失效请求本应该命中缓存,并计算这些额外失效请求带来的延迟开销,从而量化共享缓存容量干扰对 CARalone 的影响。

通过这两步,ASM 能够较为准确地估算每个应用的 CARalone,进而估算应用的延迟。

与以往工作不同,ASM 采用聚合请求行为来量化干扰,而不是试图估算每个请求的延迟,这可以避免由于内存子系统并行带来的不确定性。实验结果显示,ASM 的延迟估算误差明显小于以往最佳方法。

最后,文章提出利用 ASM 估算的延迟来改进公平性的一些机制,如延迟感知缓存分区和存储带宽分配,这些机制的评估结果显示与其他方法相比,公平性和性能都有明显提升。

总之,这篇文章提出了一个在线和准确的 ASM 模型来估算应用程序由于共享硬件资源带来的延迟,并展示了如何利用这个模型来改进系统的公平性和其他属性。

‍
